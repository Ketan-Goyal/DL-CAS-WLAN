{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as plb\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torchvision import datasets, models\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "torch.set_printoptions(precision=4)\n",
    "np.set_printoptions(precision=4)\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for allocation and channel generation\n",
    "def ch_gen(Size_area, Num_AP, Num_samples, PL_alpha=36., PL_const=0):\n",
    "    ch_w_fading = []\n",
    "    for i in range(Num_samples):\n",
    "        loc = Size_area * (np.random.rand(Num_AP, 2) - 0.5)\n",
    "        ch_w_temp_band = []\n",
    "        dist_vec = loc.reshape(Num_AP, 1, 2) - loc\n",
    "        dist_vec = np.linalg.norm(dist_vec, axis=2)\n",
    "        dist_vec = np.maximum(dist_vec, 5)\n",
    "\n",
    "        pu_ch_gain_db = - PL_const - PL_alpha * np.log10(dist_vec)\n",
    "        pu_ch_gain = 10 ** (pu_ch_gain_db / 10)\n",
    "\n",
    "        final_ch = np.maximum(pu_ch_gain, np.exp(-30))\n",
    "\n",
    "        ch_w_fading.append(np.transpose(final_ch))\n",
    "    return np.array(ch_w_fading)\n",
    "\n",
    "\n",
    "\n",
    "def rand_alloc(num_sample, num_user, num_chan):\n",
    "    mat_val = np.zeros((num_sample, num_user, num_chan))\n",
    "    for i in range(num_sample):\n",
    "        tot_iter = num_chan ** num_user\n",
    "        rand_sel = np.random.randint(tot_iter)\n",
    "        for j in range(num_user):\n",
    "            sel_ind = (rand_sel // num_chan ** j) % num_chan\n",
    "            mat_val[i, j, sel_ind] = 1\n",
    "    return mat_val\n",
    "\n",
    "\n",
    "def opt_alloc(chan, num_user, num_chan):\n",
    "    chan_ori = np.exp(chan * chan_std + chan_mean)\n",
    "    chan_ori = np.triu(chan_ori, 1)\n",
    "    tot_int_val = 10000\n",
    "    tot_alloc = 0\n",
    "    tot_iter = num_chan ** num_user\n",
    "    for i in range(tot_iter):\n",
    "        mat_val = np.zeros((num_user, num_chan))\n",
    "\n",
    "        for j in range(num_user):\n",
    "            sel_ind = (i // num_chan ** j) % num_chan\n",
    "            mat_val[j, sel_ind] = 1\n",
    "\n",
    "        cur_int_val = np.sum(np.sum(np.matmul(chan_ori, mat_val) * mat_val, 1), 0)\n",
    "        if cur_int_val < tot_int_val:\n",
    "            tot_alloc = mat_val\n",
    "            tot_int_val = cur_int_val\n",
    "\n",
    "\n",
    "    return tot_alloc, tot_int_val\n",
    "\n",
    "\n",
    "def heu_alloc(chan, num_user, num_chan):\n",
    "    chan_ori = np.exp(chan * chan_std + chan_mean)\n",
    "    chan_ori = np.triu(chan_ori, 1)\n",
    "    chan_lower = 1000*np.tri(num_user, num_user)\n",
    "    chan_temp  = chan_ori + chan_lower\n",
    "\n",
    "    mat_val = np.zeros((num_user, num_chan))\n",
    "    sel_val = 0\n",
    "    chan_idx = 0\n",
    "    round_count = True\n",
    "\n",
    "    sel_idx_mat = np.zeros(num_user)\n",
    "\n",
    "    ## Atleast two channels are assigned for each channel\n",
    "    if num_user > num_chan * 2:\n",
    "        for i in range(num_chan):\n",
    "            result = np.where(chan_temp == np.amin(chan_temp))\n",
    "            mat_val[result[0], i] = 1\n",
    "            mat_val[result[1], i] = 1\n",
    "\n",
    "            chan_temp[result[0], :] = 1000\n",
    "            chan_temp[result[1], :] = 1000\n",
    "\n",
    "            chan_temp[:, result[0]] = 1000\n",
    "            chan_temp[:, result[1]] = 1000\n",
    "\n",
    "            sel_idx_mat[result[0]] = 1\n",
    "            sel_idx_mat[result[1]] = 1\n",
    "\n",
    "        remain_usr = num_user - 2 * num_chan\n",
    "\n",
    "        for i in range(remain_usr):\n",
    "            cur_low_idx = 0\n",
    "            cur_low_val = 100000\n",
    "            idx_ch_temp = i % num_chan\n",
    "            for j in range(num_user):\n",
    "                mat_val_temp = mat_val.copy()\n",
    "                if sel_idx_mat[j] == 0:\n",
    "                    mat_val_temp[j, idx_ch_temp] = 1\n",
    "                    temp_cur_val = np.sum(np.sum(np.matmul(chan_ori, mat_val_temp) * mat_val_temp, 1), 0)\n",
    "                    if temp_cur_val < cur_low_val:\n",
    "                        cur_low_idx = j\n",
    "                        cur_low_val = temp_cur_val\n",
    "\n",
    "            mat_val[cur_low_idx, idx_ch_temp] = 1\n",
    "            sel_idx_mat[cur_low_idx] = 1\n",
    "\n",
    "    else:\n",
    "        sel_chan = 0\n",
    "        remain_usr = num_user - 2 * sel_chan\n",
    "        remain_ch = num_chan - sel_chan\n",
    "        while remain_usr != remain_ch:\n",
    "            result = np.where(chan_temp == np.amin(chan_temp))\n",
    "            mat_val[result[0], sel_chan] = 1\n",
    "            mat_val[result[1], sel_chan] = 1\n",
    "\n",
    "            chan_temp[result[0], :] = 1000\n",
    "            chan_temp[result[1], :] = 1000\n",
    "\n",
    "            chan_temp[:, result[0]] = 1000\n",
    "            chan_temp[:, result[1]] = 1000\n",
    "\n",
    "            sel_idx_mat[result[0]] = 1\n",
    "            sel_idx_mat[result[1]] = 1\n",
    "\n",
    "            sel_chan += 1\n",
    "            remain_usr = num_user - 2 * sel_chan\n",
    "            remain_ch = num_chan - sel_chan\n",
    "\n",
    "        for i in range(remain_usr):\n",
    "            sel_flag = True\n",
    "            for j in range(num_user):\n",
    "                if (sel_idx_mat[j] == 0) & (sel_flag):\n",
    "                    mat_val[j, sel_chan] = 1\n",
    "                    sel_idx_mat[j] = 1\n",
    "                    sel_flag = False\n",
    "            sel_chan += 1\n",
    "\n",
    "    tot_alloc = np.sum(np.sum(np.matmul(chan_ori, mat_val) * mat_val, 1), 0)\n",
    "\n",
    "    return mat_val, tot_alloc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Downlink calculations\n",
    "def cal_DL(chan, mat_val, chan_mean, chan_std):\n",
    "    chan_ori = np.exp(chan * chan_std + chan_mean)\n",
    "    chan_triu = np.triu(chan_ori, 1)\n",
    "    cur_int_val = np.sum(np.sum(np.matmul(chan_triu, mat_val)*mat_val, 2), 1)\n",
    "    return cur_int_val\n",
    "\n",
    "# neural network definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_user, num_chan, hidden_dim, num_l):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_user = num_user\n",
    "        self.num_chan = num_chan\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_l = num_l\n",
    "\n",
    "        ## List of linear layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(int((num_user) * (num_user-1) / 2), hidden_dim))\n",
    "        for lll in range(num_l):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "        ## List of dropout layers\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for lll in range(num_l):\n",
    "            self.dropouts.append(nn.Dropout(0.1))\n",
    "\n",
    "        ## List of Batch layers\n",
    "        self.batches = nn.ModuleList()\n",
    "        for lll in range(num_l):\n",
    "            self.batches.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        ## output for softmax and sigmoid\n",
    "        self.out_RA = nn.Linear(hidden_dim, num_user * num_chan)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = torch.flatten(x, 1)  # flatten all dimensions except the batch dimension\n",
    "        for (layer, dropout, batch_norm) in zip(self.layers, self.dropouts, self.batches):\n",
    "            # x = F.relu(nlayer(x))\n",
    "            z = layer(z)\n",
    "            z = batch_norm(z)\n",
    "            z = F.relu(z)\n",
    "            z = dropout(z)\n",
    "\n",
    "        u = self.out_RA(z)\n",
    "        u = u.view(-1, self.num_user, self.num_chan)\n",
    "        out_RA = F.softmax(u, dim=2)\n",
    "        return out_RA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def my_loss(output, chan, chan_mean, chan_std):\n",
    "    chan_ori = torch.exp(chan*chan_std + chan_mean)\n",
    "    chan_ori = torch.triu(chan_ori, diagonal=1)\n",
    "    int_sum = torch.matmul(chan_ori, output)*output\n",
    "    int_sum = torch.sum(torch.sum(int_sum, 2), 1)\n",
    "    return torch.log(int_sum.mean())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ChanDataset(Dataset):\n",
    "    def __init__(self, loc_val):\n",
    "        super(ChanDataset, self).__init__()\n",
    "        self.x_data = torch.Tensor(loc_val)\n",
    "        self.x_data = torch.triu(self.x_data, diagonal=1)\n",
    "        temp_val = []\n",
    "        idx = torch.triu_indices(*self.x_data[0].shape, 1)\n",
    "        for i in range(loc_val.shape[0]):\n",
    "            temp_val.append(self.x_data[i, idx[0], idx[1]])\n",
    "        self.y_data = torch.stack(temp_val, 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return (x, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TOT_CCI_TOT = []\n",
    "\n",
    "for jj in range(1):\n",
    "\n",
    "    net_size = 50\n",
    "    num_user = 5 * (jj+2)\n",
    "    num_chan = 4\n",
    "    tx_px = 10 ** (3.0)\n",
    "\n",
    "    tot_sample_tr = int(10 ** 4)\n",
    "    tot_sample_te = int(1 * 10 ** 1)\n",
    "\n",
    "    loc_val_tr = ch_gen(net_size, num_user, tot_sample_tr, PL_alpha=38., PL_const=34.5)\n",
    "    loc_val_te = ch_gen(net_size, num_user, tot_sample_te, PL_alpha=38., PL_const=34.5)\n",
    "\n",
    "    loc_val_tr_db = np.log(loc_val_tr)\n",
    "    loc_val_te_db = np.log(loc_val_te)\n",
    "\n",
    "    chan_mean = np.mean(loc_val_tr_db)\n",
    "    chan_std = np.std(loc_val_tr_db)\n",
    "\n",
    "    loc_val_tr_norm = (loc_val_tr_db - chan_mean) / chan_std\n",
    "    loc_val_te_norm = (loc_val_te_db - chan_mean) / chan_std\n",
    "\n",
    "    tr_dataset = ChanDataset(loc_val_tr_norm)\n",
    "    val_dataset = ChanDataset(loc_val_te_norm)\n",
    "\n",
    "    train_dataloader = DataLoader(tr_dataset, batch_size=1024*8, shuffle=True)\n",
    "    test_dataloader = DataLoader(val_dataset, batch_size=tot_sample_te, shuffle=False)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    net = Net(num_user, num_chan, 3000, 6).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.1*1e-5)\n",
    "\n",
    "\n",
    "    TOT_CCI_TEMP = []\n",
    "    for i in range(2):\n",
    "        net.train()\n",
    "        cur_loss_tot = 0\n",
    "        for index, (chan, chan_triu) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            cur_loss = my_loss(net(chan_triu.to(device)), chan.to(device), chan_mean, chan_std)\n",
    "            cur_loss_tot += cur_loss\n",
    "            cur_loss.backward()\n",
    "            optimizer.step()\n",
    "            #if index % 100 == 0:\n",
    "            #    print(\"index = \", i, \"curloss\", cur_loss_tot.detach() / (index + 1))\n",
    "            #    print(\"\")\n",
    "\n",
    "        net.eval()\n",
    "        if i%25 == 0:\n",
    "            for index, (chan_val, chan_triu_val) in enumerate(test_dataloader):\n",
    "                dl_sel = net(chan_triu_val.to(device))\n",
    "                chan_np = chan_val.cpu().detach().numpy()\n",
    "                dl_per = cal_DL(chan_np, dl_sel.cpu().detach().numpy(), chan_mean, chan_std)\n",
    "                print(\"DL per (during training) = \", 10*np.log10(np.mean(dl_per) / 2))\n",
    "\n",
    "\n",
    "    ## Inference phase\n",
    "    for index, (chan_val, chan_triu_val) in enumerate(test_dataloader):\n",
    "\n",
    "        # opt_val_tot = 0\n",
    "        # for jj in range(tot_sample_te):\n",
    "        #     _, opt_val_temp = opt_alloc(chan_np[jj], num_user, num_chan)\n",
    "        #     opt_val_tot += opt_val_temp\n",
    "        # print(\"Opt per = \", 10*np.log10(opt_val_tot / 2 / tot_sample_te))\n",
    "        # print(\"\")\n",
    "\n",
    "        # TOT_CCI_TEMP.append(10*np.log10(opt_val_tot / 2 / tot_sample_te))\n",
    "\n",
    "\n",
    "        # heu_val_tot = 0\n",
    "        # for jj in range(tot_sample_te):\n",
    "        #     _, heu_val_temp = heu_alloc(chan_np[jj], num_user, num_chan)\n",
    "        #     heu_val_tot += heu_val_temp\n",
    "        # print(\"Heu per = \", 10*np.log10(heu_val_tot / 2 / tot_sample_te))\n",
    "        # print(\"\")\n",
    "\n",
    "        # TOT_CCI_TEMP.append(10*np.log10(heu_val_tot / 2 / tot_sample_te))\n",
    "\n",
    "        rand_alloc_val = rand_alloc(tot_sample_te, num_user, num_chan)\n",
    "        rand_per = cal_DL(chan_np, rand_alloc_val, chan_mean, chan_std)\n",
    "        print(\"Rand per = \", 10*np.log10(np.mean(rand_per) / 2))\n",
    "\n",
    "        TOT_CCI_TEMP.append(10*np.log10(np.mean(rand_per) / 2))\n",
    "\n",
    "        dl_sel = net(chan_triu_val.to(device))\n",
    "        chan_np = chan_val.cpu().detach().numpy()\n",
    "        dl_per = cal_DL(chan_np, dl_sel.cpu().detach().numpy(), chan_mean, chan_std)\n",
    "        print(\"DL per = \", 10 * np.log10(np.mean(dl_per) / 2))\n",
    "\n",
    "        TOT_CCI_TEMP.append(10 * np.log10(np.mean(dl_per) / 2))\n",
    "\n",
    "        print(\"***\"*30)\n",
    "        print(\"***\" * 30)\n",
    "        print(\"***\" * 30)\n",
    "\n",
    "\n",
    "    TOT_CCI_TOT.append(np.array(TOT_CCI_TEMP))\n",
    "\n",
    "\n",
    "print(\"CCI\")\n",
    "print(np.array(TOT_CCI_TOT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
