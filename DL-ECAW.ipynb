{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DceYvCJ5O8xo",
    "outputId": "cc41b715-855c-4ed4-f9b3-3f8f9c50f6cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f7b76025130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "# !pip install torch  matplotlib numpy torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as plb\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "# !pip install torch\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torchvision import datasets, models\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "torch.set_printoptions(precision=4)\n",
    "np.set_printoptions(precision=4)\n",
    "import itertools\n",
    "import gc\n",
    "torch.autograd.set_detect_anomaly(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXbsuYkqO8xp",
    "outputId": "e633fe99-45c0-449c-b223-0c132264d2e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_user 25\n",
      "-39.39352227186145\n",
      "3.5095777971528506\n",
      "DL per (during training) =  -134.29572381200924\n",
      "DL per (during training) =  -136.03615481475765\n",
      "DL per (during training) =  -137.24064686181958\n",
      "DL per (during training) =  -137.7572457139872\n"
     ]
    }
   ],
   "source": [
    "def mem_clean():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "mem_clean()\n",
    "def ch_gen(Size_area, Num_AP, Num_samples, PL_alpha=36., PL_const=0):\n",
    "    ch_w_fading = []\n",
    "    for i in range(Num_samples):\n",
    "        loc = Size_area * (np.random.rand(Num_AP, 2) - 0.5)\n",
    "        ch_w_temp_band = []\n",
    "        dist_vec = loc.reshape(Num_AP, 1, 2) - loc\n",
    "        dist_vec = np.linalg.norm(dist_vec, axis=2)\n",
    "        dist_vec = np.maximum(dist_vec, 5)\n",
    "        # print(\"start\")\n",
    "        # print(dist_vec)\n",
    "        # print(\"end\")\n",
    "        pu_ch_gain_db = - PL_const - PL_alpha * np.log10(dist_vec)\n",
    "        # print(pu_ch_gain_db)\n",
    "        pu_ch_gain = 10 ** (pu_ch_gain_db / 10)\n",
    "        # print(pu_ch_gain)\n",
    "        final_ch = np.maximum(pu_ch_gain, np.exp(-60))\n",
    "\n",
    "        ch_w_fading.append(np.transpose(final_ch))\n",
    "    # print(ch_w_fading,\"\\n\")\n",
    "    return np.array(ch_w_fading)\n",
    "\n",
    "\n",
    "\n",
    "def rand_alloc(num_sample, num_user, num_chan):\n",
    "    mat_val = np.zeros((num_sample, num_user, num_chan))\n",
    "    mv=mat_val\n",
    "    for i in range(num_sample):\n",
    "        tot_iter = num_chan ** num_user\n",
    "        rand_sel = np.random.randint(tot_iter)\n",
    "        for j in range(num_user):\n",
    "            sel_ind = (rand_sel // num_chan ** j) % num_chan\n",
    "            mat_val[i, j, sel_ind] = 1\n",
    "            mv[i,j,sel_ind]=1\n",
    "            if sel_ind > 0:\n",
    "                mat_val[i,j,sel_ind-1]=0.005\n",
    "            if sel_ind < num_chan-1:\n",
    "                mat_val[i,j,sel_ind+1]=0.005\n",
    "#     print(mat_val)\n",
    "    return mat_val,mv\n",
    "\n",
    "\n",
    "def opt_alloc(chan, num_user, num_chan):\n",
    "    chan_ori = np.exp(chan * chan_std + chan_mean)\n",
    "    chan_ori = np.triu(chan_ori, 1)\n",
    "    tot_int_val = 10000\n",
    "    tot_alloc = 0\n",
    "    tot_iter = num_chan ** num_user\n",
    "    for i in range(tot_iter):\n",
    "        mat_val = np.zeros((num_user, num_chan))\n",
    "        mv=mat_val\n",
    "        for j in range(num_user):\n",
    "            sel_ind = (i // num_chan ** j) % num_chan\n",
    "            mat_val[j, sel_ind] = 1\n",
    "            \n",
    "        cur_int_val = np.sum(np.sum(np.matmul(chan_ori, mat_val) * mat_val, 1), 0)\n",
    "        if cur_int_val < tot_int_val:\n",
    "            tot_alloc = mat_val\n",
    "            tot_int_val = cur_int_val\n",
    "\n",
    "\n",
    "    return tot_alloc, tot_int_val\n",
    "\n",
    "\n",
    "def heu_alloc(chan, num_user, num_chan):\n",
    "    chan_ori = np.exp(chan * chan_std + chan_mean)\n",
    "    chan_ori = np.triu(chan_ori, 1)\n",
    "    chan_lower = 1000*np.tri(num_user, num_user)\n",
    "    chan_temp  = chan_ori + chan_lower\n",
    "\n",
    "    mat_val = np.zeros((num_user, num_chan))\n",
    "    sel_val = 0\n",
    "    chan_idx = 0\n",
    "    round_count = True\n",
    "\n",
    "    sel_idx_mat = np.zeros(num_user)\n",
    "\n",
    "    ## Atleast two channels are assigned for each channel\n",
    "    if num_user > num_chan * 2:\n",
    "        for i in range(num_chan):\n",
    "            result = np.where(chan_temp == np.amin(chan_temp))\n",
    "            mat_val[result[0], i] = 1\n",
    "            mat_val[result[1], i] = 1\n",
    "\n",
    "            chan_temp[result[0], :] = 1000\n",
    "            chan_temp[result[1], :] = 1000\n",
    "\n",
    "            chan_temp[:, result[0]] = 1000\n",
    "            chan_temp[:, result[1]] = 1000\n",
    "\n",
    "            sel_idx_mat[result[0]] = 1\n",
    "            sel_idx_mat[result[1]] = 1\n",
    "\n",
    "        remain_usr = num_user - 2 * num_chan\n",
    "\n",
    "        for i in range(remain_usr):\n",
    "            cur_low_idx = 0\n",
    "            cur_low_val = 100000\n",
    "            idx_ch_temp = i % num_chan\n",
    "            for j in range(num_user):\n",
    "                mat_val_temp = mat_val.copy()\n",
    "                if sel_idx_mat[j] == 0:\n",
    "                    mat_val_temp[j, idx_ch_temp] = 1\n",
    "                    temp_cur_val = np.sum(np.sum(np.matmul(chan_ori, mat_val_temp) * mat_val_temp, 1), 0)\n",
    "                    if temp_cur_val < cur_low_val:\n",
    "                        cur_low_idx = j\n",
    "                        cur_low_val = temp_cur_val\n",
    "\n",
    "            mat_val[cur_low_idx, idx_ch_temp] = 1\n",
    "            sel_idx_mat[cur_low_idx] = 1\n",
    "\n",
    "    else:\n",
    "        sel_chan = 0\n",
    "        remain_usr = num_user - 2 * sel_chan\n",
    "        remain_ch = num_chan - sel_chan\n",
    "        while remain_usr != remain_ch:\n",
    "            result = np.where(chan_temp == np.amin(chan_temp))\n",
    "            mat_val[result[0], sel_chan] = 1\n",
    "            mat_val[result[1], sel_chan] = 1\n",
    "\n",
    "            chan_temp[result[0], :] = 1000\n",
    "            chan_temp[result[1], :] = 1000\n",
    "\n",
    "            chan_temp[:, result[0]] = 1000\n",
    "            chan_temp[:, result[1]] = 1000\n",
    "\n",
    "            sel_idx_mat[result[0]] = 1\n",
    "            sel_idx_mat[result[1]] = 1\n",
    "\n",
    "            sel_chan += 1\n",
    "            remain_usr = num_user - 2 * sel_chan\n",
    "            remain_ch = num_chan - sel_chan\n",
    "\n",
    "        for i in range(remain_usr):\n",
    "            sel_flag = True\n",
    "            for j in range(num_user):\n",
    "                if (sel_idx_mat[j] == 0) & (sel_flag):\n",
    "                    mat_val[j, sel_chan] = 1\n",
    "                    sel_idx_mat[j] = 1\n",
    "                    sel_flag = False\n",
    "            sel_chan += 1\n",
    "    mv = mat_val\n",
    "    for i in range(mat_val.shape[0]):\n",
    "        for j in range(mat_val.shape[1]):\n",
    "            if mat_val[i][j]==1:\n",
    "                if j > 0:\n",
    "                    mat_val[i][j-1]=0.005\n",
    "                if j < mat_val.shape[1]-1:\n",
    "                    mat_val[i][j+1]=0.005\n",
    "                \n",
    "    tot_alloc = np.sum(np.sum(np.matmul(chan_ori, mat_val) * mv, 1), 0)\n",
    "#     print(mat_val)\n",
    "    return mat_val, tot_alloc\n",
    "\n",
    "def cal_DL_ran(chan, mat_val, mv, chan_mean, chan_std):\n",
    "    # Normalize channel gains\n",
    "#     print(mat_val)\n",
    "    chan_ori = np.exp(chan * chan_std + chan_mean)\n",
    "    chan_triu = np.triu(chan_ori, 1)\n",
    "    cur_int_val = np.zeros(chan.shape[0])\n",
    "    int_alloc = np.matmul(chan_triu, mat_val) * mv\n",
    "    int_alloc = np.sum(np.sum(int_alloc, axis=-1), axis=-1)\n",
    "    cur_int_val += int_alloc    \n",
    "    return cur_int_val\n",
    "\n",
    "def cal_DL(chan, mat_val, chan_mean, chan_std):\n",
    "    # Normalize channel gains\n",
    "#     print(mat_val)\n",
    "    _ , max_indices = torch.max(mat_val, dim=2, keepdim=True)\n",
    "#         print(max_indices[0,0,0])\n",
    "#         print(max_indices)\n",
    "    out_RA_one_hot = torch.zeros_like(mat_val,device=device)\n",
    "    out_RA_one_hot.scatter_(2, max_indices, 1)\n",
    "    mv=out_RA_one_hot.clone()\n",
    "    adjacent_update = torch.zeros_like(mat_val,device=device)\n",
    "\n",
    "# Compute indices for updating adjacent elements\n",
    "    update_indices_left = torch.clamp(max_indices - 1, min=0)\n",
    "    update_indices_right = torch.clamp(max_indices + 1, max=mat_val.size(2) - 1)\n",
    "\n",
    "    # Update adjacent elements using vectorized operations\n",
    "    adjacent_update.scatter_(2, update_indices_left, 0.005)\n",
    "    adjacent_update.scatter_(2, update_indices_right, 0.005)\n",
    "\n",
    "    # Adjust updates for edge cases\n",
    "    # If max_indices is at the beginning, set left update to 0\n",
    "    adjacent_update[:, :, 0] *= (max_indices[:, :, 0] != 0).float()\n",
    "    # If max_indices is at the end, set right update to 0\n",
    "    adjacent_update[:, :, -1] *= (max_indices[:, :, 0] != (mat_val.size(2) - 1)).float()\n",
    "\n",
    "    # Add the adjacent update to the one-hot encoded tensor\n",
    "    out_RA_one_hot = out_RA_one_hot + adjacent_update\n",
    "    out_RA_one_hot.requires_grad = True\n",
    "    \n",
    "    \n",
    "    chan_ori = torch.exp(chan * chan_std + chan_mean)\n",
    "    chan_triu = torch.triu(chan_ori, 1)\n",
    "    cur_int_val = torch.zeros(chan.shape[0],device=device)\n",
    "    int_alloc = torch.matmul(chan_triu, out_RA_one_hot) * mv\n",
    "    int_alloc = torch.sum(torch.sum(int_alloc, axis=-1), axis=-1)\n",
    "    cur_int_val += int_alloc\n",
    "    return cur_int_val\n",
    "\n",
    "\n",
    "# neural network definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_user, num_chan, hidden_dim, num_l):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_user = num_user\n",
    "        self.num_chan = num_chan\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_l = num_l\n",
    "\n",
    "        ## List of linear layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(int((num_user) * (num_user-1) / 2), hidden_dim))\n",
    "        for lll in range(num_l):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "        ## List of dropout layers\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for lll in range(num_l):\n",
    "            self.dropouts.append(nn.Dropout(0.1))\n",
    "\n",
    "        ## List of Batch layers\n",
    "        self.batches = nn.ModuleList()\n",
    "        for lll in range(num_l):\n",
    "            self.batches.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        ## output for softmax and sigmoid\n",
    "        self.out_RA = nn.Linear(hidden_dim, num_user * num_chan)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = torch.flatten(x, 1)  # flatten all dimensions except the batch dimension\n",
    "        for (layer, dropout, batch_norm) in zip(self.layers, self.dropouts, self.batches):\n",
    "              z = layer(z)\n",
    "              z = batch_norm(z)\n",
    "              z = F.relu(z)\n",
    "              z = dropout(z)\n",
    "\n",
    "        u = self.out_RA(z)\n",
    "        u = u.view(-1, self.num_user, self.num_chan)\n",
    "        out_RA = torch.softmax(u, dim=2)  # Use torch.softmax instead of F.softmax\n",
    "        # print(out_RA_one_hot)\n",
    "        return out_RA\n",
    "\n",
    "\n",
    "\n",
    "def my_loss(mat_val, chan, chan_mean, chan_std):\n",
    "\n",
    "    _ , max_indices = torch.max(mat_val, dim=2, keepdim=True)\n",
    "#         print(max_indices[0,0,0])\n",
    "#         print(max_indices)\n",
    "    out_RA_one_hot = torch.zeros_like(mat_val,device=device)\n",
    "    out_RA_one_hot.scatter_(2, max_indices, 1)\n",
    "    mv=out_RA_one_hot.clone()\n",
    "    adjacent_update = torch.zeros_like(mat_val,device=device)\n",
    "\n",
    "# Compute indices for updating adjacent elements\n",
    "    update_indices_left = torch.clamp(max_indices - 1, min=0)\n",
    "    update_indices_right = torch.clamp(max_indices + 1, max=mat_val.size(2) - 1)\n",
    "\n",
    "    # Update adjacent elements using vectorized operations\n",
    "    adjacent_update.scatter_(2, update_indices_left, 0.005)\n",
    "    adjacent_update.scatter_(2, update_indices_right, 0.005)\n",
    "\n",
    "    # Adjust updates for edge cases\n",
    "    # If max_indices is at the beginning, set left update to 0\n",
    "    adjacent_update[:, :, 0] *= (max_indices[:, :, 0] != 0).float()\n",
    "    # If max_indices is at the end, set right update to 0\n",
    "    adjacent_update[:, :, -1] *= (max_indices[:, :, 0] != (mat_val.size(2) - 1)).float()\n",
    "\n",
    "    # Add the adjacent update to the one-hot encoded tensor\n",
    "    out_RA_one_hot = out_RA_one_hot + adjacent_update\n",
    "    out_RA_one_hot.requires_grad = True\n",
    "\n",
    "    chan_ori = torch.exp(chan*chan_std + chan_mean)\n",
    "    chan_ori = torch.triu(chan_ori, diagonal=1)\n",
    "    int_sum = torch.matmul(chan_ori, mat_val)*mat_val\n",
    "    int_sum = torch.sum(torch.sum(int_sum, 2), 1)\n",
    "    return torch.log(int_sum.mean())\n",
    "\n",
    "# def my_loss(mat_val, chan, chan_mean, chan_std):\n",
    "#     chan_ori = torch.exp(chan*chan_std + chan_mean)\n",
    "#     chan_ori = torch.triu(chan_ori, diagonal=1)\n",
    "#     int_sum = torch.matmul(chan_ori, mat_val)*mat_val\n",
    "#     int_sum = torch.sum(torch.sum(int_sum, 2), 1)\n",
    "#     return torch.log(int_sum.mean())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ChanDataset(Dataset):\n",
    "    def __init__(self, loc_val):\n",
    "        super(ChanDataset, self).__init__()\n",
    "        self.x_data = torch.Tensor(loc_val)\n",
    "        self.x_data = torch.triu(self.x_data, diagonal=1)\n",
    "        temp_val = []\n",
    "        idx = torch.triu_indices(*self.x_data[0].shape, 1)\n",
    "        for i in range(loc_val.shape[0]):\n",
    "            temp_val.append(self.x_data[i, idx[0], idx[1]])\n",
    "        self.y_data = torch.stack(temp_val, 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return (x, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TOT_CCI_TOT = []\n",
    "\n",
    "for jj in range(5):\n",
    "\n",
    "    net_size=50\n",
    "    num_user=8\n",
    "    num_chan=5\n",
    "#     net_size = 10*(jj+1)\n",
    "#     print(\"net_size\",net_size)\n",
    "    num_user = 5 * (jj+1)\n",
    "    print(\"num_user\",num_user)\n",
    "#     num_chan = 2+jj\n",
    "#     print(\"num_chan\",num_chan)\n",
    "    tx_px = 10 * (3.0)\n",
    "\n",
    "\n",
    "    tot_sample_tr = int(10 ** 4)\n",
    "    tot_sample_te = int(10 ** 2)\n",
    "\n",
    "    # loc_val_tr = ch_gen(net_size, num_user, tot_sample_tr, PL_alpha=38., PL_const=34.5)\n",
    "    # loc_val_te = ch_gen(net_size, num_user, tot_sample_te, PL_alpha=38., PL_const=34.5)\n",
    "    loc_val_tr = ch_gen(net_size, num_user, tot_sample_tr, PL_alpha=55., PL_const=98)\n",
    "    loc_val_te = ch_gen(net_size, num_user, tot_sample_te, PL_alpha=55., PL_const=98)\n",
    "\n",
    "    # print(\"loc_chan_tr\",loc_val_tr)\n",
    "\n",
    "    loc_val_tr_db = np.log(loc_val_tr)\n",
    "    # print(loc_val_tr_db)\n",
    "    # print(\"loc_chan_tr_db\",loc_val_tr_db)\n",
    "    # print(loc_val_tr_db)\n",
    "    loc_val_te_db = np.log(loc_val_te)\n",
    "\n",
    "    chan_mean = np.mean(loc_val_tr_db)\n",
    "    chan_std = np.std(loc_val_tr_db)\n",
    "\n",
    "    print(chan_mean)\n",
    "    print(chan_std)\n",
    "    loc_val_tr_norm = (loc_val_tr_db - chan_mean) / chan_std\n",
    "    loc_val_te_norm = (loc_val_te_db - chan_mean) / chan_std\n",
    "\n",
    "    tr_dataset = ChanDataset(loc_val_tr_norm)\n",
    "    val_dataset = ChanDataset(loc_val_te_norm)\n",
    "\n",
    "    train_dataloader = DataLoader(tr_dataset, batch_size=2048, shuffle=True)\n",
    "    test_dataloader = DataLoader(val_dataset, batch_size=tot_sample_te, shuffle=False)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     device='cpu'\n",
    "    net = Net(num_user, num_chan, 3000, 6).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.1*1e-4)\n",
    "\n",
    "    TOT_CCI_TEMP = []\n",
    "    for i in range(300):\n",
    "        \n",
    "        net.train()\n",
    "        cur_loss_tot = 0\n",
    "        for index, (chan, chan_triu) in enumerate(train_dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            chan_triu_gpu=chan_triu.to(device)\n",
    "            chan_gpu=chan.to(device)\n",
    "            \n",
    "            mat_val=net(chan_triu_gpu)\n",
    "#             start=time.time() \n",
    "            cur_loss = my_loss(mat_val, chan_gpu, chan_mean, chan_std)\n",
    "#             print(\"time for 1 epochs = \", time.time()-start,\"s\")\n",
    "            cur_loss_tot += cur_loss\n",
    "            cur_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #if index % 100 == 0:\n",
    "            #    print(\"index = \", i, \"curloss\", cur_loss_tot.detach() / (index + 1))\n",
    "            #    print(\"\")\n",
    "        \n",
    "        net.eval()\n",
    "        if i%25 == 0:\n",
    "            for index, (chan_val, chan_triu_val) in enumerate(test_dataloader):\n",
    "                dl_sel = net(chan_triu_val.to(device))\n",
    "                chan_np = chan_val.clone()\n",
    "                \n",
    "                dl_per = cal_DL(chan_np.to(device),dl_sel, chan_mean, chan_std)\n",
    "                del dl_sel\n",
    "                dl_per=dl_per.cpu().detach().numpy()\n",
    "                print(\"DL per (during training) = \", 10*np.log10(np.mean(dl_per) / 2))\n",
    "\n",
    "    ## Inference phase\n",
    "    for index, (chan_val, chan_triu_val) in enumerate(test_dataloader):\n",
    "\n",
    "        # opt_val_tot = 0\n",
    "        # for jj in range(tot_sample_te):\n",
    "        #     _, opt_val_temp = opt_alloc(chan_np[jj], num_user, num_chan)\n",
    "        #     opt_val_tot += opt_val_temp\n",
    "        # print(\"Opt per = \", 10*np.log10(opt_val_tot / 2 / tot_sample_te))\n",
    "        # print(\"\")\n",
    "        # TOT_CCI_TEMP.append(10*np.log10(opt_val_tot / 2 / tot_sample_te))\n",
    "\n",
    "        heu_val_tot = 0\n",
    "        for jj in range(tot_sample_te):\n",
    "            mat_val, heu_val_temp = heu_alloc(chan_np[jj], num_user, num_chan)\n",
    "            # print(mat_val)\n",
    "            heu_val_tot += heu_val_temp\n",
    "        print(\"Heu per = \", 10*np.log10(heu_val_tot / 2 / tot_sample_te))\n",
    "        # print(\"\")\n",
    "        TOT_CCI_TEMP.append(10*np.log10(heu_val_tot / 2 / tot_sample_te))\n",
    "\n",
    "\n",
    "        rand_alloc_val,mv = rand_alloc(tot_sample_te, num_user, num_chan)\n",
    "        # print(rand_alloc_val)\n",
    "        rand_per = cal_DL_ran(chan_np, rand_alloc_val,mv, chan_mean, chan_std)\n",
    "        print(\"Rand per = \", 10*np.log10(np.mean(rand_per) / 2))\n",
    "        TOT_CCI_TEMP.append(10*np.log10(np.mean(rand_per) / 2))\n",
    "\n",
    "\n",
    "        dl_sel = net(chan_triu_val.to(device))\n",
    "        chan_np = chan_val.clone()\n",
    "        # print(dl_sel.cpu().detach().numpy())\n",
    "        dl_per = cal_DL(chan_np.to(device), dl_sel, chan_mean, chan_std)\n",
    "        dl_per = dl_per.cpu().detach().numpy()\n",
    "        print(\"DL per = \", 10 * np.log10(np.mean(dl_per) / 2))\n",
    "        net.cpu()\n",
    "        del net,mat_val,mv,chan_np\n",
    "        mem_clean()\n",
    "        TOT_CCI_TEMP.append(10 * np.log10(np.mean(dl_per) / 2))\n",
    "\n",
    "\n",
    "        print(\"***\" * 30)\n",
    "        print(\"***\" * 30)\n",
    "        print(\"***\" * 30)\n",
    "\n",
    "\n",
    "    TOT_CCI_TOT.append(np.array(TOT_CCI_TEMP))\n",
    "\n",
    "\n",
    "print(\"CCI\")\n",
    "print(np.array(TOT_CCI_TOT))\n",
    "\n",
    "\n",
    "\n",
    "df=pd.DataFrame(TOT_CCI_TOT)\n",
    "df.to_csv(\"user_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAOScEW9WQMI"
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "Wz1NKwjNXA0S",
    "outputId": "14a412c4-d31f-41c8-fa08-374885d6a19a"
   },
   "outputs": [],
   "source": [
    "#dl_sel.cpu()\n",
    "#del dl_sel\n",
    "# net.cpu()\n",
    "# del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jmIk16mGWQNT",
    "outputId": "c4c631ad-b04d-4e1b-ae2a-e005e92fb7d1"
   },
   "outputs": [],
   "source": [
    "def memory_stats():\n",
    "    print(torch.cuda.memory_allocated()/1024**2)\n",
    "    print(torch.cuda.memory_reserved()/1024**2)\n",
    "memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
